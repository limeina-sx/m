"""

Contributed by Wenbin Li & Jinglin Xu
å¤šè§†å›¾æ•°æ®åŠ è½½ï¼šå¤„ç†å…·æœ‰å¤šä¸ªç‰¹å¾è§†å›¾çš„æ•°æ®é›†

ç»“æ„åŒ–æ•°æ®ç»„ç»‡ï¼šæŒ‰ç…§ç±»åˆ«ç»„ç»‡æ•°æ®ï¼Œç¡®ä¿æ¯ä¸ªæ ·æœ¬æœ‰å¤šä¸ªç‰¹å¾è¡¨ç¤º

æ•°æ®åˆ’åˆ†ï¼šå°†æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æŒ‰æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†

ç‰¹å¾åŠ è½½ï¼šä»æ–‡æœ¬æ–‡ä»¶ä¸­åŠ è½½ç‰¹å¾å¹¶è½¬æ¢ä¸ºPyTorchå¼ é‡

æ¥å£æ ‡å‡†åŒ–ï¼šå®ç°äº†PyTorchæ•°æ®é›†çš„æ ‡å‡†æ¥å£ï¼ˆ__len__å’Œ__getitem__ï¼‰
"""

import os
import os.path as path
import json
import torch
import torch.utils.data as data
import numpy as np
import random
import torch.nn.functional as F
import matplotlib.pyplot as plt
plt.switch_backend('Agg')
from PIL import Image
from scipy.sparse import diags
from scipy.sparse.linalg import eigsh
from scipy.linalg import eigh
import warnings
import sys
import pickle  # æ–°å¢:ç”¨äºç¼“å­˜
import hashlib  # æ–°å¢:ç”¨äºç”Ÿæˆç¼“å­˜key
from multiprocessing import Pool, cpu_count  # æ–°å¢:ç”¨äºå¹¶è¡Œå¤„ç†
from functools import partial  # æ–°å¢:ç”¨äºä¼ é€’é¢å¤–å‚æ•°
warnings.filterwarnings('ignore')  # å¿½ç•¥è­¦å‘Šä¿¡æ¯
torch.multiprocessing.set_sharing_strategy('file_system')
#è®¾ç½®PyTorchå¤šè¿›ç¨‹å…±äº«ç­–ç•¥ä¸º'file_system'ï¼Œè¿™æ˜¯ä¸ºäº†è§£å†³å¤šè¿›ç¨‹æ•°æ®åŠ è½½ä¸­çš„å…±äº«å†…å­˜é—®é¢˜

def process_single_view_wrapper(args):
    """
    å¹¶è¡Œå¤„ç†çš„åŒ…è£…å‡½æ•°(ç”¨äºmultiprocessing.Pool)
    ç”±äºPool.mapåªèƒ½ä¼ é€’ä¸€ä¸ªå‚æ•°,éœ€è¦å°†æ‰€æœ‰å‚æ•°æ‰“åŒ…
    """
    view_idx, feats_np, params = args
    
    # è½¬æ¢numpyæ•°ç»„ä¸ºtensor
    feats = torch.from_numpy(feats_np).float()
    
    # è°ƒç”¨åŸå§‹å¤„ç†å‡½æ•°
    result = process_features_to_diffusion(
        feats=feats,
        view_idx=view_idx,
        **params  # è§£åŒ…å‚æ•°å­—å…¸
    )
    
    return view_idx, result

def compute_cache_key(feats, normalize_feats, threshold_affinity, k_ratio=0.005, which_matrix='laplacian'):
    """ç”Ÿæˆç¼“å­˜å”¯ä¸€æ ‡è¯†
    åŸºäºç‰¹å¾å½¢çŠ¶+å‚æ•°ç”Ÿæˆç¼“å­˜key(é¿å…å¯¹å…¨éƒ¨æ•°æ®è®¡ç®—hash)
    """
    # åªä½¿ç”¨å½¢çŠ¶å’Œå‚æ•°ç”Ÿæˆkey(è®¡ç®—å¿«é€Ÿ)
    shape_str = f"shape{feats.shape[0]}x{feats.shape[1]}"
    params_str = f"norm{int(normalize_feats)}_thresh{int(threshold_affinity)}_k{k_ratio}_mat{which_matrix}"
    return f"{shape_str}_{params_str}"

def process_features_to_diffusion(feats,  # è¾“å…¥ç‰¹å¾ (N, D)
                                 which_matrix='laplacian',  # çŸ©é˜µç±»å‹
                                 K=5,  # æå–çš„ç‰¹å¾å‘é‡æ•°é‡
                                 normalize_feats=True,  # æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾
                                 threshold_affinity=True,  # æ˜¯å¦è¿‡æ»¤
                                 save_dir = './results/spectrums',  # è°±å›¾ä¿å­˜ç›®å½•
                                 view_idx = 0,  # è§†å›¾ç´¢å¼•
                                 use_cache=True,  # æ–°å¢:æ˜¯å¦å¯ç”¨ç¼“å­˜
                                 cache_dir='./cache/eigenvalues'):  # æ–°å¢:ç¼“å­˜ç›®å½•
    """
    å°†è¾“å…¥ç‰¹å¾é€šè¿‡æ‰©æ•£å›¾å¤„ç†ï¼Œè¾“å‡ºæ‹‰æ™®æ‹‰æ–¯/äº²å’ŒçŸ©é˜µçš„ç‰¹å¾å‘é‡
    æ–°å¢:æ”¯æŒç¼“å­˜æœºåˆ¶ï¼Œé¦–æ¬¡è®¡ç®—åä¿å­˜ï¼Œåç»­ç›´æ¥åŠ è½½
    """
    
    # ========== ç¼“å­˜æ£€æŸ¥ ==========
    if use_cache:
        os.makedirs(cache_dir, exist_ok=True)
        cache_key = compute_cache_key(feats, normalize_feats, threshold_affinity, 0.005, which_matrix)
        cache_file = os.path.join(cache_dir, f'view_{view_idx}_{cache_key}.pkl')
        
        # å°è¯•åŠ è½½ç¼“å­˜
        if os.path.exists(cache_file):
            print(f"\n[è§†å›¾{view_idx+1}] ğŸš€ ä»ç¼“å­˜åŠ è½½ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡")
            print(f"  ç¼“å­˜æ–‡ä»¶: {os.path.basename(cache_file)}")
            sys.stdout.flush()
            
            try:
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)
                print(f"  âœ“ ç¼“å­˜åŠ è½½æˆåŠŸ (è€—æ—¶ < 0.1ç§’)")
                print(f"  - ç‰¹å¾å€¼å½¢çŠ¶: {cached_data['eigenvalues'].shape}")
                print(f"  - ç‰¹å¾å‘é‡å½¢çŠ¶: {cached_data['eigenvectors'].shape}")
                sys.stdout.flush()
                return cached_data
            except Exception as e:
                print(f"  âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
                print(f"  é‡æ–°è®¡ç®—...")
                sys.stdout.flush()
        else:
            print(f"\n[è§†å›¾{view_idx+1}] ç¼“å­˜ä¸å­˜åœ¨ï¼Œå¼€å§‹è®¡ç®—...")
            sys.stdout.flush()
    # --------------------------
    # 1. ç‰¹å¾é¢„å¤„ç†
    # --------------------------
    if normalize_feats:
        # ========== ä¿®æ”¹ï¼šä½¿ç”¨æ ‡å‡†åŒ–ä»£æ›¿L2å½’ä¸€åŒ– ==========
        # åŸå› ï¼šL2å½’ä¸€åŒ–åæ‰€æœ‰å‘é‡æ¨¡é•¿ä¸º1ï¼Œç‚¹ç§¯èŒƒå›´[-1,1]ï¼Œå¯èƒ½å¯¼è‡´ç›¸ä¼¼åº¦åˆ†å¸ƒè¿‡äºé›†ä¸­
        # æ–°æ–¹æ¡ˆï¼šä½¿ç”¨æ ‡å‡†åŒ–ï¼ˆå‡å€¼0ï¼Œæ–¹å·®1ï¼‰ï¼Œä¿ç•™ç‰¹å¾çš„åˆ†å¸ƒä¿¡æ¯
        
        # æ–¹æ¡ˆAï¼šæ ‡å‡†åŒ–ï¼ˆæ¨èï¼Œä¼˜å…ˆå°è¯•ï¼‰
        feats_mean = feats.mean(dim=0, keepdim=True)  # (1, D)
        feats_std = feats.std(dim=0, keepdim=True) + 1e-8  # (1, D)ï¼Œé¿å…é™¤é›¶
        feats = (feats - feats_mean) / feats_std  # (N, D)ï¼Œæ ‡å‡†åŒ–
        
        # æ–¹æ¡ˆBï¼šL2å½’ä¸€åŒ–ï¼ˆåŸæ–¹æ¡ˆï¼Œå¤‡é€‰ï¼‰
        # feats = F.normalize(feats, p=2, dim=-1)  # (N, D) -> (N, D)ï¼Œæ¯ä¸ªç‰¹å¾å‘é‡æ¨¡é•¿ä¸º1
        
        print(f"\n[è§†å›¾{view_idx+1}] ç‰¹å¾é¢„å¤„ç†:")
        print(f"  å½’ä¸€åŒ–æ–¹æ³•: æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰")
        print(f"  ç‰¹å¾å‡å€¼: {feats.mean():.6f}")
        print(f"  ç‰¹å¾æ ‡å‡†å·®: {feats.std():.6f}")
        sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º

    # --------------------------
    # 2. æ„å»ºäº²å’ŒçŸ©é˜µ (Affinity Matrix)
    # --------------------------
    # äº²å’ŒçŸ©é˜µè¡¨ç¤ºç‰¹å¾é—´çš„ç›¸ä¼¼åº¦ï¼Œé€šè¿‡ç‚¹ç§¯è®¡ç®—
    affinity = torch.matmul(feats, feats.T)  # (N, N)ï¼ŒA_ij = ç‰¹å¾iä¸ç‰¹å¾jçš„ç‚¹ç§¯
    
    # ========== æ–°å¢ï¼šäº²å’ŒçŸ©é˜µå½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´ ==========
    # é—®é¢˜ï¼šæ ‡å‡†åŒ–åçš„ç‚¹ç§¯èŒƒå›´æ˜¯(-âˆ, +âˆ)ï¼Œå®é™…æ•°æ®ä¸­å‡ºç°0-2047çš„å¤§æ•°å€¼
    # è§£å†³ï¼šå°†äº²å’ŒçŸ©é˜µå½’ä¸€åŒ–åˆ°[0, 1]ï¼Œé¿å…åº¦çŸ©é˜µè¿‡å¤§å¯¼è‡´ç‰¹å¾å€¼é€€åŒ–
    affinity_min = affinity.min()
    affinity_max = affinity.max()
    affinity = (affinity - affinity_min) / (affinity_max - affinity_min + 1e-8)
    
    print(f"\n[è§†å›¾{view_idx+1}] äº²å’ŒçŸ©é˜µå½’ä¸€åŒ–:")
    print(f"  åŸå§‹èŒƒå›´: [{affinity_min:.6f}, {affinity_max:.6f}]")
    print(f"  å½’ä¸€åŒ–åèŒƒå›´: [{affinity.min():.6f}, {affinity.max():.6f}]")
    print(f"  å½’ä¸€åŒ–åå‡å€¼: {affinity.mean():.6f}")
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º

    if threshold_affinity:
        # ========== ä¿®å¤ï¼šä½¿ç”¨Top-Kç¨€ç–åŒ–ç­–ç•¥ï¼ˆçœŸæ­£å‡å°‘è¿æ¥ï¼‰ ==========
        # æ ¹æœ¬é—®é¢˜ï¼š
        # 1. ä¹‹å‰çš„é˜ˆå€¼è¿‡æ»¤åªæ˜¯ç½®é›¶ï¼ŒçŸ©é˜µä»ç„¶ç¨ å¯†ï¼ˆ21311x21311å…¨éƒ¨å…ƒç´ ï¼‰
        # 2. åº¦çŸ©é˜µè®¡ç®—åŒ…å«äº†æ‰€æœ‰0å€¼ï¼Œå¯¼è‡´åº¦å€¼è¿‡å¤§
        # 3. å¯¹ç§°å½’ä¸€åŒ–åå¯¹è§’çº¿æ¥è¿‘1ï¼Œç‰¹å¾å€¼å…¨éƒ¨èšé›†åœ¨1é™„è¿‘
        
        # æ–°ç­–ç•¥ï¼šæ¯ä¸ªèŠ‚ç‚¹åªä¿ç•™Kä¸ªæœ€å¼ºè¿æ¥ï¼ˆçœŸæ­£ç¨€ç–åŒ–ï¼‰
        # å‚æ•°è°ƒä¼˜ï¼šä»1%é™ä½åˆ°0.5%ï¼Œè¿›ä¸€æ­¥é™ä½åº¦å€¼ï¼Œä½¿å¯¹è§’çº¿è¿œç¦»1
        k_neighbors = max(10, int(affinity.shape[0] * 0.005))  # æ¯ä¸ªèŠ‚ç‚¹ä¿ç•™0.5%çš„è¿æ¥ï¼ˆçº¦106ä¸ªï¼‰
        
        print(f"\n[è§†å›¾{view_idx+1}] é˜ˆå€¼è¿‡æ»¤ç­–ç•¥:")
        print(f"  ç­–ç•¥ç±»å‹: Top-Kç¨€ç–åŒ–ï¼ˆæ¯èŠ‚ç‚¹ä¿ç•™æœ€å¼ºKä¸ªè¿æ¥ï¼‰")
        print(f"  Kå€¼: {k_neighbors} (æ ·æœ¬æ€»æ•°çš„0.5%)")
        print(f"  è¿‡æ»¤å‰éé›¶å…ƒç´ : {(affinity > 0).sum().item()}")
        sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
        
        # å¯¹æ¯ä¸€è¡Œä¿ç•™Top-Kæœ€å¤§å€¼
        topk_values, topk_indices = torch.topk(affinity, k_neighbors, dim=1)  # (N, K)
        
        # åˆ›å»ºç¨€ç–çŸ©é˜µï¼ˆåªä¿ç•™Top-Kè¿æ¥ï¼‰
        affinity_sparse = torch.zeros_like(affinity)  # (N, N)
        for i in range(affinity.shape[0]):
            affinity_sparse[i, topk_indices[i]] = topk_values[i]
        
        # å¯¹ç§°åŒ–ï¼ˆä¿è¯æ— å‘å›¾ï¼‰
        affinity = (affinity_sparse + affinity_sparse.T) / 2.0
        
        print(f"  è¿‡æ»¤åéé›¶å…ƒç´ : {(affinity > 0).sum().item()}")
        print(f"  ç¨€ç–åº¦: {(affinity > 0).sum().item() / affinity.numel() * 100:.2f}%")
        print(f"  ç†è®ºæœ€å¤§éé›¶å…ƒç´ : {affinity.shape[0] * k_neighbors * 2} (å¯¹ç§°åŒ–å)")
        sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º

    # è½¬æ¢ä¸ºnumpyæ•°ç»„ç”¨äºåç»­è®¡ç®—
    affinity_np = affinity.cpu().numpy()  # (N, N)
    
    # ============ è¯Šæ–­è¾“å‡ºï¼šäº²å’ŒçŸ©é˜µç»Ÿè®¡ ============
    print(f"\n[è§†å›¾{view_idx+1}] äº²å’ŒçŸ©é˜µç»Ÿè®¡:")
    print(f"  å½¢çŠ¶: {affinity_np.shape}")
    print(f"  æœ€å¤§å€¼: {affinity_np.max():.6f}")
    print(f"  æœ€å°å€¼: {affinity_np.min():.6f}")
    print(f"  å‡å€¼: {affinity_np.mean():.6f}")
    print(f"  æ ‡å‡†å·®: {affinity_np.std():.6f}")
    print(f"  éé›¶å…ƒç´ æ¯”ä¾‹: {(affinity_np > 0).mean()*100:.2f}%")
    print(f"  æ­£å€¼æ¯”ä¾‹: {(affinity_np > 0).mean()*100:.2f}%")
    print(f"  è´Ÿå€¼æ¯”ä¾‹: {(affinity_np < 0).mean()*100:.2f}%")
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º

    # --------------------------
    # 3. æ„å»ºæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ (Laplacian Matrix)
    # --------------------------
    if which_matrix == 'laplacian':
        # ========== ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—åº¦çŸ©é˜µï¼ˆåªè®¡ç®—éé›¶å…ƒç´ çš„å’Œï¼‰ ==========
        # é—®é¢˜ï¼šä¹‹å‰çš„np.sumåŒ…å«äº†æ‰€æœ‰0å€¼ï¼Œå¯¼è‡´åº¦å€¼è™šé«˜
        # è§£å†³ï¼šåªå¯¹éé›¶å…ƒç´ æ±‚å’Œï¼ˆç¨€ç–çŸ©é˜µçš„çœŸå®åº¦ï¼‰
        degree = np.sum(affinity_np, axis=1)  # (N,)ï¼Œç¨€ç–çŸ©é˜µçš„åº¦ï¼ˆ0å€¼ä¼šè¢«è‡ªåŠ¨å¿½ç•¥ï¼‰
        degree_matrix = diags(degree)  # (N, N)ï¼Œå¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸ºåº¦å€¼
        
        # ============ è¯Šæ–­è¾“å‡ºï¼šåº¦çŸ©é˜µç»Ÿè®¡ ============
        print(f"\n[è§†å›¾{view_idx+1}] åº¦çŸ©é˜µç»Ÿè®¡:")
        print(f"  æœ€å¤§åº¦: {degree.max():.6f}")
        print(f"  æœ€å°åº¦: {degree.min():.6f}")
        print(f"  å¹³å‡åº¦: {degree.mean():.6f}")
        print(f"  åº¦æ ‡å‡†å·®: {degree.std():.6f}")
        print(f"  é›¶åº¦èŠ‚ç‚¹æ•°: {(degree == 0).sum()}")
        sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º

        # æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ L = D - Aï¼ˆDä¸ºåº¦çŸ©é˜µï¼ŒAä¸ºäº²å’ŒçŸ©é˜µï¼‰
        laplacian = degree_matrix - affinity_np  # (N, N)
        
        # ============ è¯Šæ–­è¾“å‡ºï¼šæœªå½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µç»Ÿè®¡ ============
        print(f"\n[è§†å›¾{view_idx+1}] æœªå½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µç»Ÿè®¡:")
        print(f"  æœ€å¤§å€¼: {laplacian.max():.6f}")
        print(f"  æœ€å°å€¼: {laplacian.min():.6f}")
        print(f"  å‡å€¼: {laplacian.mean():.6f}")
        print(f"  å¯¹è§’çº¿å‡å€¼: {np.diag(laplacian).mean():.6f}")
        sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º

        # ========== ç»ˆæä¿®å¤ï¼šä½¿ç”¨æœªå½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ ==========
        # é—®é¢˜åˆ†æï¼š
        # 1. å¯¹ç§°å½’ä¸€åŒ–ï¼šL_sym â†’ Iï¼ˆå•ä½çŸ©é˜µï¼‰ï¼Œå¯¹è§’çº¿â‰ˆ1
        # 2. éšæœºæ¸¸èµ°å½’ä¸€åŒ–ï¼šL_rw â†’ Iï¼Œå¯¹è§’çº¿=1-0/d=1ï¼ˆå› ä¸ºA_ii=0ï¼‰
        # 3. ç»“è®ºï¼šä»»ä½•å½’ä¸€åŒ–åœ¨ç¨€ç–å›¾ä¸Šéƒ½ä¼šå¯¼è‡´å¯¹è§’çº¿æ¥è¿‘1ï¼Œç‰¹å¾å€¼é€€åŒ–
        # 
        # è§£å†³æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨æœªå½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ L = D - A
        # - å¯¹è§’çº¿ï¼šç­‰äºåº¦å€¼ï¼ˆä¸æ˜¯1ï¼‰
        # - ç‰¹å¾å€¼èŒƒå›´ï¼š[0, 2*d_max]ï¼ˆæœ‰çœŸå®çš„åˆ†å¸ƒï¼‰
        # - ç¼ºç‚¹ï¼šä¸åŒè§†å›¾å°ºåº¦ä¸åŒï¼ˆä½†å¯ä»¥æ¥å—ï¼‰
        
        # ä¸åšä»»ä½•å½’ä¸€åŒ–ï¼Œç›´æ¥ä½¿ç”¨ L = D - A
        # ï¼ˆlaplacianå·²ç»åœ¨ä¸Šé¢è®¡ç®—å¥½äº†ï¼‰
        
        print(f"\n[è§†å›¾{view_idx+1}] æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µå½’ä¸€åŒ–:")
        print(f"  å½’ä¸€åŒ–ç±»å‹: æ— å½’ä¸€åŒ–ï¼ˆä½¿ç”¨åŸå§‹æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ L = D - Aï¼‰")
        print(f"  ç†è®ºç‰¹å¾å€¼èŒƒå›´: [0, {2*degree.max():.2f}]ï¼ˆçº¦ä¸º2å€æœ€å¤§åº¦ï¼‰")
        print(f"  ä¼˜åŠ¿: é¿å…å¯¹è§’çº¿é€€åŒ–ä¸º1ï¼Œä¿ç•™çœŸå®çš„è°±ç»“æ„")
        sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
        
        # ============ è¯Šæ–­è¾“å‡ºï¼šæœ€ç»ˆæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µç»Ÿè®¡ ============
        print(f"\n[è§†å›¾{view_idx+1}] æœ€ç»ˆæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µç»Ÿè®¡:")
        print(f"  æœ€å¤§å€¼: {laplacian.max():.6f}")
        print(f"  æœ€å°å€¼: {laplacian.min():.6f}")
        print(f"  å‡å€¼: {laplacian.mean():.6f}")
        print(f"  å¯¹è§’çº¿å‡å€¼: {np.diag(laplacian).mean():.6f}")
        print(f"  å¯¹è§’çº¿æœ€å¤§å€¼: {np.diag(laplacian).max():.6f}")
        print(f"  å¯¹è§’çº¿æœ€å°å€¼: {np.diag(laplacian).min():.6f}")
        print(f"  é¢„æœŸç‰¹å¾å€¼èŒƒå›´: [0, {laplacian.max():.2f}]")
        sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º

        matrix_to_eig = laplacian
        # æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µç‰¹å¾å€¼ä»å°åˆ°å¤§æ’åºï¼Œå–å‰Kä¸ªï¼ˆå«0ç‰¹å¾å€¼ï¼‰
        which_eig = 'SM'  # æœ€å°ç‰¹å¾å€¼

    elif which_matrix == 'affinity':
        # ç›´æ¥ä½¿ç”¨äº²å’ŒçŸ©é˜µè®¡ç®—ç‰¹å¾å‘é‡
        matrix_to_eig = affinity_np
        # äº²å’ŒçŸ©é˜µç‰¹å¾å€¼ä»å¤§åˆ°å°æ’åºï¼Œå–å‰Kä¸ª
        which_eig = 'LM'  # æœ€å¤§ç‰¹å¾å€¼

    else:
        raise ValueError("çŸ©é˜µç±»å‹å¿…é¡»ä¸º 'laplacian' æˆ– 'affinity'")

        # -------------------------- ä¿®å¤ï¼šä½¿ç”¨ç¨€ç–çŸ©é˜µeigshé¿å…å†…å­˜æº¢å‡º --------------------------
    # é—®é¢˜ï¼šnp.linalg.eighè®¡ç®—å…¨éƒ¨21311ä¸ªç‰¹å¾å€¼éœ€è¦15-25GBå†…å­˜ï¼Œå¯¼è‡´OOMï¼ˆè¿›ç¨‹è¢«Killedï¼‰
    # è§£å†³ï¼šä½¿ç”¨scipy.sparse.linalg.eigshåªè®¡ç®—å‰kä¸ªç‰¹å¾å€¼ï¼ˆçº¦500ä¸ªï¼‰ï¼Œå†…å­˜éœ€æ±‚é™è‡³<1GB
    
    n_eigenvalues_for_plot = min(300, matrix_to_eig.shape[0])  # æœ€å¤šè®¡ç®—500ä¸ªç‰¹å¾å€¼ç”¨äºç»˜å›¾
    
    print(f"\n[è§†å›¾{view_idx + 1}] å¼€å§‹è®¡ç®—ç‰¹å¾å€¼è°±ï¼ˆå‰{n_eigenvalues_for_plot}ä¸ªç‰¹å¾å€¼ï¼‰")
    print(f"  çŸ©é˜µå¤§å°: {matrix_to_eig.shape[0]}x{matrix_to_eig.shape[0]}")
    print(f"  å†…å­˜ä¼˜åŒ–: ä½¿ç”¨ç¨€ç–ç‰¹å¾å€¼åˆ†è§£ï¼ˆé¿å…OOMï¼‰")
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    
    # ä½¿ç”¨eigshè®¡ç®—å‰kä¸ªç‰¹å¾å€¼ï¼ˆå†…å­˜å‹å¥½ï¼‰
    try:
        if which_matrix == 'laplacian':
            # ========== æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨ARPACKç®—æ³•åªè®¡ç®—éœ€è¦çš„ç‰¹å¾å€¼ ==========
            # åŸæ–¹æ¡ˆï¼šè®¡ç®—å…¨éƒ¨21311ä¸ªç‰¹å¾å€¼ -> è€—æ—¶2-5åˆ†é’Ÿ
            # ä¼˜åŒ–æ–¹æ¡ˆï¼šä½¿ç”¨scipy.sparse.linalg.eigsh + shift-invertæ¨¡å¼ -> è€—æ—¶10-30ç§’
            
            # è½¬æ¢ä¸ºç¨€ç–çŸ©é˜µæ ¼å¼ï¼ˆCSRæ ¼å¼ï¼Œeigshä¸“ç”¨ï¼‰
            from scipy.sparse import csr_matrix
            matrix_sparse = csr_matrix(matrix_to_eig) if not hasattr(matrix_to_eig, 'toarray') else matrix_to_eig
            
            print(f"  çŸ©é˜µè½¬æ¢: å¯†é›†/ç¨€ç–æ··åˆ -> CSRç¨€ç–çŸ©é˜µï¼ˆ{matrix_sparse.shape}ï¼‰")
            sys.stdout.flush()
            
            # ä½¿ç”¨shift-invertæ¨¡å¼åŠ é€Ÿè®¡ç®—æœ€å°ç‰¹å¾å€¼
            # sigma=0: å¯»æ‰¾æ¥è¿‘0çš„ç‰¹å¾å€¼ï¼ˆæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„æœ€å°ç‰¹å¾å€¼ï¼‰
            # mode='normal': æ ‡å‡†æ¨¡å¼ï¼ˆæ¯”shift-invertæ›´ç¨³å®šï¼‰
            print(f"  å¼€å§‹è®¡ç®—ç‰¹å¾å€¼ï¼ˆARPACKç®—æ³•ï¼‰...")
            sys.stdout.flush()
            
            all_eigenvalues = eigsh(matrix_sparse, k=n_eigenvalues_for_plot, which='SM', 
                                   return_eigenvectors=False, maxiter=10000, tol=1e-3)
            all_eigenvalues = np.sort(all_eigenvalues)  # å‡åºæ’åˆ—
            
            print(f"  âœ“ ç‰¹å¾å€¼è®¡ç®—å®Œæˆ")
            sys.stdout.flush()
            
        else:
            # äº²å’ŒçŸ©é˜µï¼šè®¡ç®—æœ€å¤§çš„kä¸ªç‰¹å¾å€¼
            from scipy.sparse import csr_matrix
            matrix_sparse = csr_matrix(matrix_to_eig) if not hasattr(matrix_to_eig, 'toarray') else matrix_to_eig
            print(f"  å¼€å§‹è®¡ç®—ç‰¹å¾å€¼ï¼ˆARPACKç®—æ³•ï¼‰...")
            sys.stdout.flush()
            
            all_eigenvalues = eigsh(matrix_sparse, k=n_eigenvalues_for_plot, which='LM', 
                                   return_eigenvectors=False, maxiter=10000, tol=1e-3)
            all_eigenvalues = np.sort(all_eigenvalues)[::-1]  # é™åºæ’åˆ—
            
            print(f"  âœ“ ç‰¹å¾å€¼è®¡ç®—å®Œæˆ")
            sys.stdout.flush()

        # ç‰¹å¾å€¼æ’åºå¤„ç†
        if which_matrix == 'laplacian':
            # æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ:å·²ç»æ˜¯æœ€å°çš„300ä¸ª,ä¿æŒå‡åº(ä»0å¼€å§‹)
            all_eigenvalues_sorted = all_eigenvalues  # å‡åº:[0, 2.3, 3.9, ..., 8.2]
        else:
            # äº²å’ŒçŸ©é˜µ:å·²ç»æ˜¯æœ€å¤§çš„300ä¸ª,ä¿æŒé™åº
            all_eigenvalues_sorted = all_eigenvalues  # é™åº

        print(f"  æˆåŠŸè®¡ç®—{len(all_eigenvalues_sorted)}ä¸ªç‰¹å¾å€¼")
        sys.stdout.flush()
        
    except Exception as e:
        print(f"  è­¦å‘Šï¼šeigshå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¯†é›†çŸ©é˜µæ–¹æ³•ï¼ˆå¯èƒ½å†…å­˜ä¸è¶³ï¼‰")
        print(f"  é”™è¯¯ä¿¡æ¯: {e}")
        sys.stdout.flush()
        
        # å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœçŸ©é˜µè¾ƒå°æˆ–eigshå¤±è´¥ï¼Œä½¿ç”¨eigh
        if matrix_to_eig.shape[0] <= 5000:
            all_eigenvalues = np.linalg.eigh(matrix_to_eig)[0]
            all_eigenvalues_sorted = np.sort(all_eigenvalues)[::-1][:n_eigenvalues_for_plot]
        else:
            print(f"  é”™è¯¯ï¼šçŸ©é˜µè¿‡å¤§ä¸”eigshå¤±è´¥ï¼Œè·³è¿‡ç‰¹å¾è°±è®¡ç®—")
            sys.stdout.flush()
            # è¿”å›ç©ºç»“æœï¼Œè·³è¿‡ç»˜å›¾
            return {
                'eigenvalues': torch.zeros(K).float(),
                'eigenvectors': torch.zeros(K, feats.shape[0]).float(),
                'affinity': affinity,
                'laplacian': matrix_to_eig if which_matrix == 'laplacian' else None
            }
    
    # ============ è¯Šæ–­è¾“å‡º:ç‰¹å¾å€¼ç»Ÿè®¡ ============
    # ç»Ÿä¸€å˜é‡åï¼šæ‰€æœ‰åç»­ä»£ç ä½¿ç”¨all_eigenvalues
    all_eigenvalues_sorted = all_eigenvalues
    
    print(f"\n[è§†å›¾{view_idx+1}] ç‰¹å¾å€¼ç»Ÿè®¡(å‰{len(all_eigenvalues_sorted)}ä¸ª):")
    print(f"  ç‰¹å¾å€¼æ•°é‡(è®¡ç®—çš„): {len(all_eigenvalues_sorted)}")
    print(f"  ç‰¹å¾å€¼æ•°é‡(æ€»å…±): {matrix_to_eig.shape[0]}")
        
    if which_matrix == 'laplacian':
        # æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ:å‡åºæ’åˆ—,æœ€å°å€¼åœ¨å‰
        print(f"  æœ€å°ç‰¹å¾å€¼: {all_eigenvalues_sorted[0]:.6f}  (åº”æ¥è¿‘0)")
        print(f"  æœ€å¤§ç‰¹å¾å€¼(å‰{len(all_eigenvalues_sorted)}ä¸ªä¸­): {all_eigenvalues_sorted[-1]:.6f}")
    else:
        # äº²å’ŒçŸ©é˜µ:é™åºæ’åˆ—,æœ€å¤§å€¼åœ¨å‰
        print(f"  æœ€å¤§ç‰¹å¾å€¼: {all_eigenvalues_sorted[0]:.6f}")
        print(f"  æœ€å°ç‰¹å¾å€¼(å‰{len(all_eigenvalues_sorted)}ä¸ªä¸­): {all_eigenvalues_sorted[-1]:.6f}")
        
    print(f"  ç‰¹å¾å€¼å‡å€¼: {all_eigenvalues_sorted.mean():.6f}")
    print(f"  ç‰¹å¾å€¼æ ‡å‡†å·®: {all_eigenvalues_sorted.std():.6f}")
    print(f"  æ¥è¿‘1çš„ç‰¹å¾å€¼æ•°é‡(0.99-1.01): {((all_eigenvalues_sorted > 0.99) & (all_eigenvalues_sorted < 1.01)).sum()}")
    print(f"  æ¥è¿‘0çš„ç‰¹å¾å€¼æ•°é‡(<0.01): {(np.abs(all_eigenvalues_sorted) < 0.01).sum()}")
    print(f"  å‰10ä¸ªç‰¹å¾å€¼: {all_eigenvalues_sorted[:10]}")
    print(f"  å10ä¸ªç‰¹å¾å€¼: {all_eigenvalues_sorted[-10:]}")
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    
    # ============ è¯Šæ–­è¾“å‡º:ç‰¹å¾å€¼åˆ†å¸ƒç»Ÿè®¡ ============
    print(f"\n[è§†å›¾{view_idx+1}] ç‰¹å¾å€¼åˆ†å¸ƒ:")
    if which_matrix == 'laplacian':
        print(f"  ç¬¬1ä¸ªç‰¹å¾å€¼(æœ€å°): {all_eigenvalues_sorted[0]:.6f}")
        print(f"  ç¬¬2ä¸ªç‰¹å¾å€¼: {all_eigenvalues_sorted[1]:.6f}")
        print(f"  ç¬¬10ä¸ªç‰¹å¾å€¼: {all_eigenvalues_sorted[min(9, len(all_eigenvalues_sorted)-1)]:.6f}")
        print(f"  ç¬¬100ä¸ªç‰¹å¾å€¼: {all_eigenvalues_sorted[min(99, len(all_eigenvalues_sorted)-1)]:.6f}")
        print(f"  ç¬¬{len(all_eigenvalues_sorted)}ä¸ªç‰¹å¾å€¼(æœ€å¤§): {all_eigenvalues_sorted[-1]:.6f}")
    else:
        print(f"  ç¬¬1ä¸ªç‰¹å¾å€¼(æœ€å¤§): {all_eigenvalues_sorted[0]:.6f}")
        print(f"  ç¬¬2ä¸ªç‰¹å¾å€¼: {all_eigenvalues_sorted[1]:.6f}")
        print(f"  ç¬¬10ä¸ªç‰¹å¾å€¼: {all_eigenvalues_sorted[min(9, len(all_eigenvalues_sorted)-1)]:.6f}")
        print(f"  ç¬¬100ä¸ªç‰¹å¾å€¼: {all_eigenvalues_sorted[min(99, len(all_eigenvalues_sorted)-1)]:.6f}")
        print(f"  ç¬¬{len(all_eigenvalues_sorted)}ä¸ªç‰¹å¾å€¼(æœ€å°): {all_eigenvalues_sorted[-1]:.6f}")
    print(f"  ç‰¹å¾å€¼æ–¹å·®: {np.var(all_eigenvalues_sorted):.6f}")
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    # 3. åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆè‹¥ä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
    os.makedirs(save_dir, exist_ok=True)
    # 4. ç»˜åˆ¶è°±å›¾
    plt.figure(figsize=(10, 6))
    # ç»˜åˆ¶ç‰¹å¾å€¼åˆ†å¸ƒæ›²çº¿ï¼ˆæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µä»å°åˆ°å¤§ï¼Œç›´æ¥ç»˜åˆ¶ï¼‰
    plt.plot(range(1, len(all_eigenvalues_sorted) + 1),  # æ¨ªè½´ï¼šç‰¹å¾å€¼ç´¢å¼•ï¼ˆ1å¼€å§‹ï¼‰
             all_eigenvalues_sorted,
             marker='.', linestyle='-', color='darkblue', alpha=0.7, markersize=2)
    # æ·»åŠ é›¶å€¼è¾…åŠ©çº¿ï¼ˆæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µç‰¹å¾å€¼éè´Ÿï¼Œé›¶å€¼çº¿å¯è¾…åŠ©åˆ¤æ–­â€œèšç±»ç°‡æ•°â€ï¼‰
    plt.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Zero Threshold')
    # å›¾è¡¨æ ‡æ³¨
    plt.xlabel('Eigenvalue Index (Sorted Descending)', fontsize=12)
    plt.ylabel('Eigenvalue Value', fontsize=12)
    plt.title(f'Feature Spectrum (View {view_idx + 1}, {which_matrix} Matrix)', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€ï¼Œé¿å…æ ‡ç­¾è¢«æˆªæ–­
    # 5. ä¿å­˜è°±å›¾ï¼ˆæ–‡ä»¶åå«è§†å›¾ç´¢å¼•ï¼Œé¿å…å¤šè§†å›¾è¦†ç›–ï¼‰
    spectrum_path = os.path.join(save_dir,
                                 f'view_{view_idx + 1}_{which_matrix}_spectrum.png')
    plt.savefig(spectrum_path, dpi=300)  # dpi=300ç¡®ä¿å›¾ç‰‡æ¸…æ™°åº¦
    plt.close()  # å…³é—­ç”»å¸ƒï¼Œé‡Šæ”¾å†…å­˜
    print(f"è§†å›¾{view_idx + 1}ï¼šç‰¹å¾è°±å›¾å·²ä¿å­˜åˆ° {spectrum_path}")
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    
    # ========================== æ–°å¢ï¼šç»˜åˆ¶ä¸­é—´çŸ©é˜µçƒ­åŠ›å›¾ ==========================
    print(f"\n[è§†å›¾{view_idx + 1}] å¼€å§‹ç»˜åˆ¶ä¸­é—´çŸ©é˜µçƒ­åŠ›å›¾...")
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    
    # ä¸ºäº†å¯è§†åŒ–æ•ˆæœï¼Œå¯¹å¤§çŸ©é˜µè¿›è¡Œé‡‡æ ·ï¼ˆåªæ˜¾ç¤ºéƒ¨åˆ†æ ·æœ¬ï¼‰
    n_samples = affinity_np.shape[0]
    max_display = 100  # æœ€å¤šæ˜¾ç¤º100x100çš„çƒ­åŠ›å›¾ï¼Œé¿å…å›¾ç‰‡è¿‡å¤§
    
    if n_samples > max_display:
        # éšæœºé‡‡æ ·ç´¢å¼•
        sample_indices = np.sort(np.random.choice(n_samples, max_display, replace=False))
        affinity_display = affinity_np[np.ix_(sample_indices, sample_indices)]
        degree_display = degree[sample_indices]
        laplacian_display = matrix_to_eig[np.ix_(sample_indices, sample_indices)] if which_matrix == 'laplacian' else None
        display_info = f"(é‡‡æ ·{max_display}/{n_samples}ä¸ªæ ·æœ¬)"
    else:
        affinity_display = affinity_np
        degree_display = degree
        laplacian_display = matrix_to_eig if which_matrix == 'laplacian' else None
        display_info = f"(å…¨éƒ¨{n_samples}ä¸ªæ ·æœ¬)"
    
    # åˆ›å»ºçƒ­åŠ›å›¾ä¿å­˜ç›®å½•
    heatmap_dir = os.path.join(save_dir, 'heatmaps')
    os.makedirs(heatmap_dir, exist_ok=True)
    
    # ------- 1. ç»˜åˆ¶äº²å’ŒçŸ©é˜µçƒ­åŠ›å›¾ -------
    plt.figure(figsize=(10, 8))
    im = plt.imshow(affinity_display, cmap='viridis', aspect='auto', interpolation='nearest')
    plt.colorbar(im, label='Affinity Value')
    plt.title(f'Affinity Matrix Heatmap - View {view_idx + 1}\n{display_info}', fontsize=14)
    plt.xlabel('Sample Index', fontsize=12)
    plt.ylabel('Sample Index', fontsize=12)
    plt.tight_layout()
    affinity_heatmap_path = os.path.join(heatmap_dir, f'view_{view_idx + 1}_affinity_heatmap.png')
    plt.savefig(affinity_heatmap_path, dpi=200)
    plt.close()
    print(f"  âœ“ äº²å’ŒçŸ©é˜µçƒ­åŠ›å›¾å·²ä¿å­˜: {affinity_heatmap_path}")
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    
    # ------- 2. ç»˜åˆ¶åº¦åˆ†å¸ƒç›´æ–¹å›¾ -------
    plt.figure(figsize=(10, 6))
    plt.hist(degree_display, bins=50, color='steelblue', edgecolor='black', alpha=0.7)
    plt.axvline(degree_display.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {degree_display.mean():.2f}')
    plt.axvline(degree_display.min(), color='green', linestyle='--', linewidth=2, label=f'Min: {degree_display.min():.2f}')
    plt.axvline(degree_display.max(), color='orange', linestyle='--', linewidth=2, label=f'Max: {degree_display.max():.2f}')
    plt.xlabel('Degree Value', fontsize=12)
    plt.ylabel('Frequency', fontsize=12)
    plt.title(f'Degree Distribution - View {view_idx + 1}\n{display_info}', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    degree_hist_path = os.path.join(heatmap_dir, f'view_{view_idx + 1}_degree_distribution.png')
    plt.savefig(degree_hist_path, dpi=200)
    plt.close()
    print(f"  âœ“ åº¦åˆ†å¸ƒç›´æ–¹å›¾å·²ä¿å­˜: {degree_hist_path}")
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    
    # ------- 3. ç»˜åˆ¶æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçƒ­åŠ›å›¾ï¼ˆä»…å½“ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µæ—¶ï¼‰ -------
    if which_matrix == 'laplacian' and laplacian_display is not None:
        plt.figure(figsize=(10, 8))
        im = plt.imshow(laplacian_display, cmap='RdBu_r', aspect='auto', interpolation='nearest', vmin=0, vmax=2)
        plt.colorbar(im, label='Laplacian Value')
        plt.title(f'Normalized Laplacian Matrix Heatmap - View {view_idx + 1}\n{display_info}', fontsize=14)
        plt.xlabel('Sample Index', fontsize=12)
        plt.ylabel('Sample Index', fontsize=12)
        plt.tight_layout()
        laplacian_heatmap_path = os.path.join(heatmap_dir, f'view_{view_idx + 1}_laplacian_heatmap.png')
        plt.savefig(laplacian_heatmap_path, dpi=200)
        plt.close()
        print(f"  âœ“ æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçƒ­åŠ›å›¾å·²ä¿å­˜: {laplacian_heatmap_path}")
        sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    
    # ------- 4. ç»˜åˆ¶äº²å’ŒçŸ©é˜µç¨€ç–æ¨¡å¼å›¾ -------
    plt.figure(figsize=(10, 8))
    # ä½¿ç”¨äºŒå€¼åŒ–æ˜¾ç¤ºç¨€ç–æ¨¡å¼ï¼ˆéé›¶ä¸º1ï¼Œé›¶ä¸º0ï¼‰
    sparse_pattern = (affinity_display > 0).astype(float)
    im = plt.imshow(sparse_pattern, cmap='binary', aspect='auto', interpolation='nearest')
    plt.colorbar(im, label='Connection (0=No, 1=Yes)', ticks=[0, 1])
    plt.title(f'Affinity Sparsity Pattern - View {view_idx + 1}\n{display_info}\nNon-zero: {(sparse_pattern > 0).sum()}/{sparse_pattern.size} ({(sparse_pattern > 0).mean()*100:.2f}%)', fontsize=14)
    plt.xlabel('Sample Index', fontsize=12)
    plt.ylabel('Sample Index', fontsize=12)
    plt.tight_layout()
    sparsity_path = os.path.join(heatmap_dir, f'view_{view_idx + 1}_sparsity_pattern.png')
    plt.savefig(sparsity_path, dpi=200)
    plt.close()
    print(f"  âœ“ ç¨€ç–æ¨¡å¼å›¾å·²ä¿å­˜: {sparsity_path}")
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    
    # ------- 5. ç»¼åˆç»Ÿè®¡å¯¹æ¯”å›¾ -------
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    
    # å­å›¾1ï¼šäº²å’ŒçŸ©é˜µå€¼åˆ†å¸ƒç›´æ–¹å›¾
    axes[0, 0].hist(affinity_display.flatten(), bins=100, color='teal', edgecolor='black', alpha=0.7)
    axes[0, 0].axvline(affinity_display.mean(), color='red', linestyle='--', label=f'Mean: {affinity_display.mean():.4f}')
    axes[0, 0].set_xlabel('Affinity Value')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].set_title('Affinity Value Distribution')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # å­å›¾2ï¼šåº¦å€¼åˆ†å¸ƒï¼ˆç®±çº¿å›¾ï¼‰
    axes[0, 1].boxplot(degree_display, vert=True, patch_artist=True,
                       boxprops=dict(facecolor='lightblue', color='blue'),
                       medianprops=dict(color='red', linewidth=2))
    axes[0, 1].set_ylabel('Degree Value')
    axes[0, 1].set_title(f'Degree Distribution (Boxplot)\nMean={degree_display.mean():.2f}, Std={degree_display.std():.2f}')
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    
    # å­å›¾3ï¼šç‰¹å¾å€¼åˆ†å¸ƒï¼ˆå‰50ä¸ªï¼‰
    top_k_eig = min(50, len(all_eigenvalues_sorted))
    axes[1, 0].bar(range(1, top_k_eig + 1), all_eigenvalues_sorted[:top_k_eig], color='navy', alpha=0.7)
    axes[1, 0].set_xlabel('Eigenvalue Rank')
    axes[1, 0].set_ylabel('Eigenvalue')
    axes[1, 0].set_title(f'Top {top_k_eig} Eigenvalues')
    axes[1, 0].grid(True, alpha=0.3, axis='y')
    
    # å­å›¾4ï¼šçŸ©é˜µç»Ÿè®¡æ‘˜è¦ï¼ˆæ–‡æœ¬ï¼‰
    axes[1, 1].axis('off')
    summary_text = f"""Matrix Statistics Summary (View {view_idx + 1})

ã€Affinity Matrixã€‘
  Shape: {affinity_np.shape}
  Max: {affinity_np.max():.6f}
  Min: {affinity_np.min():.6f}
  Mean: {affinity_np.mean():.6f}
  Std: {affinity_np.std():.6f}
  Non-zero ratio: {(affinity_np > 0).mean()*100:.2f}%

ã€Degree Matrixã€‘
  Max degree: {degree.max():.2f}
  Min degree: {degree.min():.2f}
  Mean degree: {degree.mean():.2f}
  Std degree: {degree.std():.2f}

ã€Eigenvalue Spectrumã€‘
  Max eigenvalue: {all_eigenvalues.max():.6f}
  Min eigenvalue: {all_eigenvalues.min():.6f}
  Mean eigenvalue: {all_eigenvalues.mean():.6f}
  Std eigenvalue: {all_eigenvalues.std():.6f}
  Eigenvalues â‰ˆ 1: {((all_eigenvalues > 0.99) & (all_eigenvalues < 1.01)).sum()}
    """
    axes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes,
                    fontsize=10, verticalalignment='top', family='monospace',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    summary_path = os.path.join(heatmap_dir, f'view_{view_idx + 1}_summary.png')
    plt.savefig(summary_path, dpi=200)
    plt.close()
    print(f"  âœ“ ç»¼åˆç»Ÿè®¡å›¾å·²ä¿å­˜: {summary_path}")
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    
    print(f"\n[è§†å›¾{view_idx + 1}] æ‰€æœ‰çƒ­åŠ›å›¾å·²ä¿å­˜è‡³: {heatmap_dir}")
    print("=" * 70)
    sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    # 4. ç‰¹å¾å‘é‡æå–ï¼ˆæ‰©æ•£å›¾æ ¸å¿ƒï¼‰
    # --------------------------
    # è®¡ç®—çŸ©é˜µçš„å‰Kä¸ªç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
    # eigshé€‚ç”¨äºå¯¹ç§°çŸ©é˜µï¼Œæ•ˆç‡é«˜äºæ™®é€šç‰¹å¾å€¼åˆ†è§£
    eigenvalues, eigenvectors = eigsh(matrix_to_eig, k=K, which=which_eig)  # (K,), (N, K)

    # è°ƒæ•´ç‰¹å¾å‘é‡é¡ºåºï¼ˆæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µæŒ‰ä»å°åˆ°å¤§ï¼Œäº²å’ŒçŸ©é˜µæŒ‰ä»å¤§åˆ°å°ï¼‰
    if which_matrix == 'laplacian':
        # æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µç‰¹å¾å€¼ä»å°åˆ°å¤§ï¼Œå¯¹åº”æ‰©æ•£å›¾çš„"ä½é¢‘"åˆ†é‡
        eigenvectors = eigenvectors[:, :K]  # (N, K)
    else:
        # äº²å’ŒçŸ©é˜µç‰¹å¾å€¼ä»å¤§åˆ°å°ï¼Œå¯¹åº”æœ€æ˜¾è‘—çš„å…³è”æ¨¡å¼
        eigenvectors = eigenvectors[:, ::-1]  # åè½¬é¡ºåºï¼Œç¡®ä¿ä»å¤§åˆ°å°

    # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶è°ƒæ•´ç»´åº¦ï¼ˆK, Nï¼‰
    eigenvectors = torch.from_numpy(eigenvectors.T).float()  # (K, N)

    # --------------------------
    # 5. ç‰¹å¾å‘é‡åå¤„ç†ï¼ˆè§£å†³ç¬¦å·æ­§ä¹‰ï¼‰
    # --------------------------
    for k in range(K):
        # è‹¥ç‰¹å¾å‘é‡ä¸­æ­£å€¼å æ¯”è¶…è¿‡50%ï¼Œåè½¬ç¬¦å·ï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
        if torch.mean((eigenvectors[k] > 0).float()) > 0.5:
            eigenvectors[k] = -eigenvectors[k]

    # æ„å»ºè¿”å›ç»“æœ
    result_data = {
        'eigenvalues': torch.from_numpy(eigenvalues).float(),  # ç‰¹å¾å€¼
        'eigenvectors': eigenvectors,  # ç‰¹å¾å‘é‡ï¼ˆæ‰©æ•£å›¾çš„æ ¸å¿ƒè¾“å‡ºï¼‰
        'affinity': affinity,  # äº²å’ŒçŸ©é˜µ
        'laplacian': laplacian if which_matrix == 'laplacian' else None  # æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼ˆè‹¥è®¡ç®—ï¼‰
    }
    
    # ========== ä¿å­˜åˆ°ç¼“å­˜ ==========
    if use_cache:
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(result_data, f, protocol=pickle.HIGHEST_PROTOCOL)
            print(f"\n[è§†å›¾{view_idx+1}] âœ“ ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡å·²ç¼“å­˜")
            print(f"  ç¼“å­˜æ–‡ä»¶: {os.path.basename(cache_file)}")
            print(f"  æ–‡ä»¶å¤§å°: {os.path.getsize(cache_file) / (1024**2):.2f} MB")
            sys.stdout.flush()
        except Exception as e:
            print(f"  âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            sys.stdout.flush()

    return result_data

class animalAttrData(object):
    """
       Dataloader for animal attributes dataset.ç±»å®šä¹‰å’Œåˆå§‹åŒ–
    """
    def __init__(self, data_dir='/home/limn/mvnn/Mvnn/mvdata/AWA/Features',
        mode='train',
        diffusion_K = 10,
        diffusion_matrix = 'laplacian',
        normalize_feats = True,
        threshold_affinity = True,
        save_spectrum_dir='./results/spectrums',
        use_cache=True,  # æ–°å¢:ç¼“å­˜æ§åˆ¶
        cache_dir='./cache/eigenvalues',  # æ–°å¢:ç¼“å­˜ç›®å½•
        use_parallel=True,  # æ–°å¢:æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
        num_workers=None):  # æ–°å¢:å¹¶è¡Œè¿›ç¨‹æ•°(é»˜è®¤ä¸ºCPUæ ¸æ•°)

        print(type(object))
        super(animalAttrData, self).__init__()
        self.diffusion_K = diffusion_K
        self.diffusion_matrix = diffusion_matrix
        self.normalize_feats = normalize_feats
        self.threshold_affinity = threshold_affinity
        self.save_spectrum_dir = save_spectrum_dir  # ä¿å­˜è°±å›¾ç›®å½•åˆ°å®ä¾‹å˜é‡
        self.use_cache = use_cache  # ç¼“å­˜æ§åˆ¶
        self.cache_dir = cache_dir  # ç¼“å­˜ç›®å½•
        self.use_parallel = use_parallel  # å¹¶è¡Œæ§åˆ¶
        self.num_workers = num_workers if num_workers is not None else min(cpu_count(), 6)  # é»˜è®¤æœ€å¤š6ä¸ªè¿›ç¨‹
        """. æ•°æ®åŠ è½½å’Œåˆ’åˆ†é€»è¾‘    1. æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„è·¯å¾„å’Œæ ‡ç­¾"""
        data_list = []#åˆå§‹åŒ–ç©ºåˆ—è¡¨data_listç”¨äºå­˜å‚¨æ‰€æœ‰æ•°æ®æ ·æœ¬
        fea_name = os.listdir(data_dir)
        #è·å–ç‰¹å¾æ–‡ä»¶å¤¹åç§°åˆ—è¡¨ï¼ˆä¸åŒè§†å›¾çš„ç‰¹å¾ï¼‰
        class_name = os.listdir(os.path.join(data_dir, fea_name[0]))
        #è·å–ç±»åˆ«åç§°åˆ—è¡¨ï¼ˆä»ç¬¬ä¸€ä¸ªç‰¹å¾æ–‡ä»¶å¤¹ä¸­è·å–ï¼‰

        count = -1   
        for class_item in class_name:
        #éå†æ¯ä¸ªç±»åˆ«ï¼Œä¸ºæ¯ä¸ªç±»åˆ«åˆ†é…ä¸€ä¸ªå”¯ä¸€ç´¢å¼•ï¼ˆcountï¼‰
            count += 1
            class_list = []
            class_path_list = []
            #ä¸ºå½“å‰ç±»åˆ«åˆ›å»ºç©ºåˆ—è¡¨class_listå’Œclass_path_list
            for fea_item in fea_name:
                class_path_list.append(os.path.join(data_dir, fea_item, class_item))
                #æ„å»ºæ¯ä¸ªç‰¹å¾è§†å›¾çš„è·¯å¾„åˆ—è¡¨

            sample_name = os.listdir(class_path_list[0])

            # each sample have servel kinds of features
            for sample_item in sample_name:#è·å–å½“å‰ç±»åˆ«ä¸‹çš„æ‰€æœ‰æ ·æœ¬åç§°
                
                sample_fea_all = [os.path.join(class_path_list[i], sample_item) for i in range(len(class_path_list))]
                #ä¸ºæ¯ä¸ªæ ·æœ¬æ„å»ºæ‰€æœ‰ç‰¹å¾è§†å›¾çš„å®Œæ•´è·¯å¾„åˆ—è¡¨
                class_list.append((sample_fea_all, count))
                #å°†(ç‰¹å¾è·¯å¾„åˆ—è¡¨, ç±»åˆ«ç´¢å¼•)å…ƒç»„æ·»åŠ åˆ°ç±»åˆ«åˆ—è¡¨ä¸­
            
            # divide the data into training set and testing set
            random.seed(int(100)) 
            train_part = random.sample(class_list, int(0.7*len(class_list)) )
            rem_part = [rem for rem in class_list if rem not in train_part]
            val_part = random.sample(rem_part, int(2/3.0*len(rem_part)))
            test_part = [te for te in rem_part if te not in val_part]
#è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
#å°†å½“å‰ç±»åˆ«çš„æ ·æœ¬åˆ’åˆ†ä¸ºï¼šè®­ç»ƒé›†ï¼š70%   éªŒè¯é›†ï¼šå‰©ä½™æ ·æœ¬çš„2/3ï¼ˆçº¦20%ï¼‰  æµ‹è¯•é›†ï¼šå‰©ä½™æ ·æœ¬çš„1/3ï¼ˆçº¦10%ï¼‰
            if mode == 'train':
                data_list.extend(train_part)
            elif mode == 'val':
                data_list.extend(val_part)
            else:
                data_list.extend(test_part)

        self.data_list = data_list
        # 3. é¢„åŠ è½½æ‰€æœ‰æ ·æœ¬çš„åŸå§‹ç‰¹å¾ï¼ŒæŒ‰è§†å›¾ç»„ç»‡
        num_views = len(fea_name)
        self.view_features = [[] for _ in range(num_views)]  # æ¯ä¸ªè§†å›¾çš„ç‰¹å¾åˆ—è¡¨ï¼š[è§†å›¾0ç‰¹å¾, è§†å›¾1ç‰¹å¾, ...]
        for sample_fea_all, _ in self.data_list:
            for v in range(num_views):
                fea_temp = np.loadtxt(sample_fea_all[v])  # åŠ è½½å•ä¸ªæ ·æœ¬çš„è§†å›¾vç‰¹å¾
                self.view_features[v].append(fea_temp)

        # 4. å¯¹æ¯ä¸ªè§†å›¾çš„ç‰¹å¾è¿›è¡Œæ‰©æ•£å›¾å¤„ç†ï¼ˆæ ¸å¿ƒæ–°å¢æ­¥éª¤ï¼‰
        self.processed_view_features = []  # å­˜å‚¨å¤„ç†åçš„ç‰¹å¾
        
        # ========== å¹¶è¡Œå¤„ç†ä¼˜åŒ– ==========
        if self.use_parallel and num_views > 1:
            print(f"\nğŸš€ å¯ç”¨å¹¶è¡Œå¤„ç†: {num_views}ä¸ªè§†å›¾ï¼Œ{self.num_workers}ä¸ªè¿›ç¨‹")
            print(f"CPUæ ¸æ•°: {cpu_count()}ï¼Œä½¿ç”¨è¿›ç¨‹æ•°: {self.num_workers}")
            sys.stdout.flush()
            
            # å‡†å¤‡å¹¶è¡Œä»»åŠ¡å‚æ•°
            tasks = []
            params = {
                'which_matrix': self.diffusion_matrix,
                'K': self.diffusion_K,
                'normalize_feats': self.normalize_feats,
                'threshold_affinity': self.threshold_affinity,
                'save_dir': self.save_spectrum_dir,
                'use_cache': self.use_cache,
                'cache_dir': self.cache_dir
            }
            
            for v in range(num_views):
                feats_np = np.array(self.view_features[v])  # è½¬ä¸ºnumpyæ•°ç»„
                tasks.append((v, feats_np, params))
            
            # å¹¶è¡Œæ‰§è¡Œ
            try:
                print(f"\nå¼€å§‹å¹¶è¡Œå¤„ç†...")
                print(f"æç¤ºï¼šå¦‚æœé•¿æ—¶é—´æ— è¾“å‡ºï¼Œè¯´æ˜æ­£åœ¨è®¡ç®—ç‰¹å¾å€¼ï¼Œè¯·è€å¿ƒç­‰å¾…")
                print(f"é¢„è®¡æ¯ä¸ªè§†å›¾éœ€è¦10-30ç§’ï¼ˆé¦–æ¬¡ï¼‰æˆ–<1ç§’ï¼ˆç¼“å­˜ï¼‰")
                sys.stdout.flush()
                
                with Pool(self.num_workers) as pool:
                    # ä½¿ç”¨imap_unorderedè·å–ç»“æœ
                    results = []
                    completed = 0
                    for result in pool.imap_unordered(process_single_view_wrapper, tasks):
                        results.append(result)
                        completed += 1
                        print(f"\n>>> è¿›åº¦: {completed}/{num_views} è§†å›¾å·²å®Œæˆ")
                        sys.stdout.flush()
                
                # æŒ‰è§†å›¾é¡ºåºæ’åºç»“æœ
                results_sorted = sorted(results, key=lambda x: x[0])
                
                for view_idx, diffusion_result in results_sorted:
                    processed_feats = diffusion_result['eigenvectors'].T  # (N, K)
                    self.processed_view_features.append(processed_feats)
                
                print(f"\nâœ“ å¹¶è¡Œå¤„ç†å®Œæˆ! æ‰€æœ‰{num_views}ä¸ªè§†å›¾å·²å¤„ç†")
                sys.stdout.flush()
                
            except Exception as e:
                print(f"\nâš ï¸ å¹¶è¡Œå¤„ç†å¤±è´¥: {e}")
                print("å›é€€åˆ°ä¸²è¡Œå¤„ç†...")
                sys.stdout.flush()
                self.use_parallel = False  # å›é€€åˆ°ä¸²è¡Œæ¨¡å¼
        
        # ========== ä¸²è¡Œå¤„ç†ï¼ˆå¤‡ç”¨ï¼‰ ==========
        if not self.use_parallel or num_views == 1:
            print(f"\nä½¿ç”¨ä¸²è¡Œå¤„ç†: {num_views}ä¸ªè§†å›¾")
            print(f"æç¤ºï¼šå¦‚æœé•¿æ—¶é—´æ— è¾“å‡ºï¼Œè¯´æ˜æ­£åœ¨è®¡ç®—ç‰¹å¾å€¼ï¼Œè¯·è€å¿ƒç­‰å¾…")
            sys.stdout.flush()
            
            for v in range(num_views):
                print(f"\n>>> å¼€å§‹å¤„ç†è§†å›¾ {v+1}/{num_views}")
                sys.stdout.flush()
                
                # è½¬æ¢ä¸ºTensorï¼š(N, D)
                feats = torch.from_numpy(np.array(self.view_features[v])).float()
                # æ‰©æ•£å›¾å¤„ç†
                diffusion_result = process_features_to_diffusion(
                    feats=feats,
                    which_matrix=self.diffusion_matrix,
                    K=self.diffusion_K,
                    normalize_feats=self.normalize_feats,
                    threshold_affinity=self.threshold_affinity,
                    save_dir=self.save_spectrum_dir,
                    view_idx=v,
                    use_cache=self.use_cache,
                    cache_dir=self.cache_dir
                )
                # æå–ç‰¹å¾å‘é‡ï¼š(K, N) -> è½¬ç½®ä¸º (N, K)
                processed_feats = diffusion_result['eigenvectors'].T  # (N, K)
                self.processed_view_features.append(processed_feats)
                
                print(f">>> è§†å›¾ {v+1}/{num_views} å¤„ç†å®Œæˆ")
                sys.stdout.flush()
#æ•°æ®åŠ è½½å™¨æ–¹æ³•
    def __len__(self):
        return len(self.data_list)
        #è¿”å›æ•°æ®é›†çš„å¤§å°ï¼ˆæ ·æœ¬æ•°é‡ï¼‰


    def __getitem__(self, index):
        '''
            Load an episode each time
        '''
        # 5. è¿”å›å¤„ç†åçš„ç‰¹å¾ï¼ˆæ›¿ä»£åŸå§‹ç‰¹å¾ï¼‰
        _, target = self.data_list[index]  # åŸå§‹æ•°æ®åˆ—è¡¨ä»…ç”¨äºè·å–æ ‡ç­¾
        processed_fea = [self.processed_view_features[v][index]
                         for v in range(len(self.processed_view_features))]
        # å…ˆæŠŠ list of 1-D numpy æ‹¼æˆ 2-D ndarray
        #fea_np = np.stack([self.processed_view_features[v][index].numpy()
        #                   for v in range(len(self.processed_view_features))], axis=0)
        # æŒ‰ç´¢å¼•æ”¶é›†æ¯ä¸ªè§†å›¾çš„å¤„ç†åç‰¹å¾
        #processed_fea = torch.from_numpy(fea_np).float()   # shape (num_view, K)
        # ---- è°ƒè¯•ä»£ç  ----
        # for v, f in enumerate(processed_fea):
        #     print(f'view{v} shape={f.shape}', end='  ')
        # print()
        # -----------------
        #processed_fea = [self.processed_view_features[v][index] for v in range(len(self.processed_view_features))]
        return processed_fea, target
        # (sample_fea_all, target) = self.data_list[index]
        # #æ ¹æ®ç´¢å¼•è·å–ä¸€ä¸ªæ•°æ®æ ·æœ¬ ä»å…ƒç»„ä¸­æå–ç‰¹å¾è·¯å¾„åˆ—è¡¨å’Œç›®æ ‡ç±»åˆ«
        # Sample_Fea = []
        # for i in range(len(sample_fea_all)):#éå†æ‰€æœ‰ç‰¹å¾è§†å›¾ï¼Œä½¿ç”¨np.loadtxtåŠ è½½ç‰¹å¾æ–‡ä»¶
        #     fea_temp = np.loadtxt(sample_fea_all[i])
        #     fea_temp = torch.from_numpy(fea_temp)
        #     #å°†NumPyæ•°ç»„è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ç¡®ä¿ä¸ºFloatç±»å‹
        #     Sample_Fea.append(fea_temp.type(torch.FloatTensor))
        #
        # return (Sample_Fea, target)
        # #è¿”å›ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«æ‰€æœ‰ç‰¹å¾è§†å›¾çš„å¼ é‡åˆ—è¡¨å’Œç›®æ ‡ç±»åˆ«
        

